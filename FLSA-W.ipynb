{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45171b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb28897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "news_df = pd.read_csv('us_equities_news_dataset.csv') \n",
    "news_df['content'] = news_df['content'].fillna('')\n",
    "texts = news_df['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c85b19f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = simple_preprocess(text)  # Tokenization and lowercasing\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5b1c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ticker                                              title release_date\n",
      "24        NIO  A Central Bank War Just Started And Its Good F...   2019-03-07\n",
      "32        NIO         6 Stocks To Watch  Nivida Could Be Falling   2019-03-06\n",
      "57        NIO  Stocks   Dow Drops Nearly 400 Points as Apple ...   2018-11-19\n",
      "78       UBER  The Zacks Analyst Blog Highlights  Advanced Mi...   2020-01-12\n",
      "82       UBER                     The Best Of CES 2020  Revised    2020-01-16\n",
      "...       ...                                                ...          ...\n",
      "221198    AMD  7 Tech Stocks With SMA50 Above SMA200 And High...   2012-12-07\n",
      "221202    AMD                           Deceased Rubber Felines    2012-11-25\n",
      "221229    PFE          Durata Therapeutics  DRTX  Pre IPO Report   2012-07-19\n",
      "221468      T  Zacks com Featured Highlights  AT T  Nu Skin E...   2016-07-21\n",
      "221471      T  5 Dividend Growth Stocks To Sail Through Uncer...   2016-07-20\n",
      "\n",
      "[7256 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Filter the news dataset for the articles that contain knowledge about NVidia.\n",
    "#These can be abbravations for Nvidia or just the name so this list contains words I'm searchinmg for in filtering the dataset\n",
    "#by checking if all content or title contain \"nvidia_terms\" list.\n",
    "nvidia_terms = [\n",
    "    'Nvidia', 'NVDA', 'N V D I A', 'Nvidia Corporation', 'NVIDIA Corp', \n",
    "    'NVIDIA GPUs', 'NVIDIA graphics', 'NVIDIA chip', 'NVIDIA cards', \n",
    "    'Nvidia GeForce', 'GeForce', 'Nvidia Quadro', 'NVIDIA AI', \n",
    "    'NVIDIA Tegra', 'Nvidia Drive', 'Nvidia RTX', 'RTX', \n",
    "    'NVIDIA DGX', 'NVIDIA Shield', 'CUDA', 'NVIDIA Jetson', \n",
    "    'NVIDIA Omniverse', 'NVIDIA Hopper', 'NVIDIA architecture',\n",
    "    'Nvidia hardware', 'Nvidia software', 'NVIDIA tech'\n",
    "]\n",
    "\n",
    "#Second filtering is applied to tickers to find the some companies that can affect the price. \n",
    "#My research shows AMD and Intel are the biggest competitors for Nvidia so I'm also including articles with ticker = 'AMD,INTC'\n",
    "filtered_news_df = news_df[\n",
    "    (news_df['title'].str.contains('|'.join(nvidia_terms), case=False, na=False)) |\n",
    "    (news_df['content'].str.contains('|'.join(nvidia_terms), case=False, na=False)) |\n",
    "    (news_df['ticker'].isin(['AMD', 'INTC']))  # Filter for AMD and Intel tickers\n",
    "]\n",
    "\n",
    "# Display the filtered DataFrame with Nvidia, AMD, and Intel related articles\n",
    "print(filtered_news_df[['ticker', 'title', 'release_date']])\n",
    "filtered_news_df = filtered_news_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea53c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_news_df = pd.read_csv('filtered_news_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c5df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_news_df.to_csv('filtered_news_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2a0e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_news_df['processed_text'] = filtered_news_df['content'].fillna('').apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1c083d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [ecb, effect, move, euro, huge, falling, pip, ...\n",
       "1    [stock, watch, march, trading, session, stock,...\n",
       "2    [investing, com, rout, apple, facebook, nasdaq...\n",
       "3    [immediate, releasechicago, il, january, zacks...\n",
       "4    [company, bringing, innovation, ce, jan, get, ...\n",
       "Name: processed_text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_news_df['processed_text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a14926b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'ticker', 'title', 'category', 'content', 'release_date',\n",
       "       'provider', 'url', 'article_id', 'processed_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "194c7525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join all preprocessed text for FLSA-W\n",
    "all_tokens = sum(filtered_news_df['processed_text'], [])  \n",
    "merged_string = ' '.join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ab2d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each list of tokens in 'processed_text' to a single string\n",
    "filtered_news_df['processed_text'] = filtered_news_df['processed_text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5870d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_news_df.to_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73a47bd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Now apply TF-IDF\u001b[39;00m\n\u001b[0;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(filtered_news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Apply TruncatedSVD for topic modeling\u001b[39;00m\n\u001b[0;32m      9\u001b[0m n_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Adjust based on the number of topics you want\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_nlp_env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_nlp_env\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_nlp_env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_nlp_env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# Now apply TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(filtered_news_df['processed_text'])\n",
    "\n",
    "# Apply TruncatedSVD for topic modeling\n",
    "n_topics = 5  # Adjust based on the number of topics you want\n",
    "svd_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "lsa_matrix = svd_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Display Topics\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for idx, topic in enumerate(svd_model.components_):\n",
    "    print(f\"Topic {idx + 1}: \", \" \".join([terms[i] for i in topic.argsort()[-10:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f295b2",
   "metadata": {},
   "source": [
    "Summary of Each Topic’s Role\n",
    "Topic 1 and Topic 2 focus on financial performance and growth metrics specifically related to Nvidia, with one leaning more towards growth metrics (Topic 1) and the other on analyst recommendations and predictions (Topic 2).\n",
    "Topic 3 covers stock trading activity and market behavior, providing insights into Nvidia’s stock price movements and trading volumes.\n",
    "Topic 4 broadens the view to include industry news and external factors affecting Nvidia and its competitors, with attention to market dynamics influenced by major tech players and geopolitical events.\n",
    "Topic 5 zeroes in on earnings reports and revenue—the financial backbone of Nvidia’s quarterly performance.\n",
    "\n",
    "Generated by chatgpt for the topics above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5e59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c316ac1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my_nlp_env] *",
   "language": "python",
   "name": "conda-env-my_nlp_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
